<!DOCTYPE HTML>
<html lang="en"><head><meta http-equiv="Content-Type" content="text/html; charset=UTF-8">

  <title>Xiaolin Hong</title>
  
  <meta name="author" content="Xiaolin Hong">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  
  <link rel="stylesheet" type="text/css" href="stylesheet.css">
	<link rel="icon" href="data:image/svg+xml,<svg xmlns=%22http://www.w3.org/2000/svg%22 viewBox=%220 0 100 100%22><text y=%22.9em%22 font-size=%2290%22>🌐</text></svg>">
</head>

<body>
  <h1 style="text-align: center;"> Learning Visually Aligned Body Motion for In-the-wild Videos </h1>
  <p style="text-align: center;"><img src="images/teaser.png" alt=""/></p>
	<h2 style="text-align: center;">Abstract</h2>
<!--	<p>The estimation of 3D human pose and shape from monocular video is a critical research area for understanding human behavior. Although significant progress has been made, this problem is still challenging due to the complex and diverse real-world human motions. Existing video-based methods often face difficulties in producing visually aligned motion sequences. These limitations can be attributed to the use of global features that lack motion information, the inability to model long-range dependencies, and the failure to capture informative human motions.-->
<!--To address these challenges, this paper proposes a novel Visually Aligned body motion Network (VisAlign-Net) that effectively learns temporal dependencies for in-the-wild videos. The proposed approach employs multi-scale spatial features to achieve image-aligned reconstruction. Moreover, an Entropy-aware Point-wise Temporal Enhancement (EPTE) module with an entropy-guided attention mechanism is introduced to capture the most relevant information. Additionally, a PArt-aware Temporal Encoder (PATE) is proposed to ensure more robust temporal continuity. This is achieved by designing separate motion reconstructions through temporal encoding for different parts of the human body, giving each part full play.-->
<!--Our method outperforms the previous state-of-the-art video-based methods on three benchmark datasets, indicating its superiority in reconstructing accurate image-aligned results. </p>-->
	<h2 style="text-align: center;">Demo</h2>
<p><video autoplay="autoplay" controls="controls">
<source src="data/v1.mp4" type="video/mp4" /></video></p>
<p><video autoplay="autoplay" controls="controls">
<source src="data/v2.mp4" type="video/mp4" /></video></p>
<p><video autoplay="autoplay" controls="controls">
<source src="data/v3.mp4" type="video/mp4" /></video></p>
<p><video autoplay="autoplay" controls="controls">
<source src="data/v4.mp4" type="video/mp4" /></video></p>
  <table>
  <tr>
    <td><video src="data/v5.mp4" controls width="250"></video></td>
    <td><video src="data/v5.mp4" controls width="250"></video></td>
  </tr>
  <tr>
    <td><video src="data/v5.mp4" controls width="250"></video></td>
    <td><video src="data/v5.mp4" controls width="250"></video></td>
  </tr>
</table>

<!--<p><video autoplay="autoplay" controls="controls">-->
<!--<source src="data/v5.mp4" type="video/mp4" /></video></p>-->
<!--<p><video autoplay="autoplay" controls="controls">-->
<!--<source src="data/v6.mp4" type="video/mp4" /></video></p>-->
</body>

</html>

